# -*- coding: utf-8 -*-
"""decisiontree_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17O4m32HEbvP0RyX-buk8TWvBbPtncGPG
"""

import numpy as np
import math


# These are suggested helper functions
# You can structure your code differently, but if you have
# trouble getting started, this might be a good starting point


# Create the decision tree recursively
def make_node(previous_ys, xs, ys, columns):
    # WARNING: lists are passed by reference in python
    # If you are planning to remove items, it's better
    # to create a copy first

    columns = columns[:]

    # First, check the three termination criteria:

    # If there are no rows (xs and ys are empty):
    #      Return a node that classifies as the majority class of the parent
    if not xs or not ys:
        return {"type": "class", "class": majority(previous_ys)}

    # If all ys are the same:
    #      Return a node that classifies as that class
    if same(ys):
        return {"type": "class", "class": ys[0]}

    # If there are no more columns left:
    #      Return a node that classifies as the majority class of the ys
    if not columns:
        return {"type": "class", "class": majority(ys)}

    # Otherwise:
    # Compute the entropy of the current ys
    current_entropy = entropy(ys)
    best_gain = -1
    best_column = None
    best_splits = None

    # For each column:
    for column in columns:
        splits = {}
        for x, y in zip(xs, ys):
            value = x[column]
            if value not in splits:
                splits[value] = {"xs": [], "ys": []}
            splits[value]["xs"].append(x)
            splits[value]["ys"].append(y)

        # Calculate the entropy of each of the pieces
        # Compute the overall entropy as the weighted sum
        split_entropy = sum(
            len(split["ys"]) / len(ys) * entropy(split["ys"])
            for split in splits.values()
        )
        # The gain of the column is the difference of the entropy before
        #    the split, and this new overall entropy
        gain = current_entropy - split_entropy

        # Select the column with the highest gain
        if gain > best_gain:
            best_gain = gain
            best_column = column
            best_splits = splits

    # If no gain, return a leaf node with majority class
    if best_gain <= 0:
        return {"type": "class", "class": majority(ys)}

    # Split the data along the column values and recursively call
    #    make_node for each piece
    # Create a split-node that splits on this column, and has the result
    #    of the recursive calls as children.
    node = {"type": "split", "attribute": best_column, "children": {}}
    remaining_columns = [col for col in columns if col != best_column]

    for value, split in best_splits.items():
        node["children"][value] = make_node(
            ys, split["xs"], split["ys"], remaining_columns
        )

    return node


# Determine if all values in a list are the same
# Useful for the second basecase above
def same(values):
    if not values:
        return True
    # if there are values:
    first_val = values[0]
    # pick the first, check if all other are the same
    return all(val == first_val for val in values)


# Determine how often each value shows up
# in a list; this is useful for the entropy
# but also to determine which values is the
# most common
def counts(values):
    count_dict = {}
    for value in values:
        if value in count_dict:
            count_dict[value] += 1
        else:
            count_dict[value] = 1
    return count_dict


# Return the most common value from a list
# Useful for base cases 1 and 3 above.
def majority(values):
    count_dict = counts(values)
    return max(count_dict, key=count_dict.get)


# Calculate the entropy of a set of values
# First count how often each value shows up
# When you divide this value by the total number
# of elements, you get the probability for that element
# The entropy is the negation of the sum of p*log2(p)
# for all these probabilities.
def entropy(values):
    count_dict = counts(values)
    total = len(values)
    ent = 0
    for count in count_dict.values():
        p = count / total
        ent -= p * math.log2(p)
    return ent


# This is the main decision tree class
# DO NOT CHANGE THE FOLLOWING LINE
class DecisionTree:
    # DO NOT CHANGE THE PRECEDING LINE
    def __init__(self, tree={}):
        self.tree = tree

    # DO NOT CHANGE THE FOLLOWING LINE
    def fit(self, x, y):
        # DO NOT CHANGE THE PRECEDING LINE

        self.majority = majority(y)
        self.tree = make_node(y, x, y, list(range(len(x[0]))))

    # DO NOT CHANGE THE FOLLOWING LINE
    def predict(self, x):
        # DO NOT CHANGE THE PRECEDING LINE
        if not self.tree:
            return None

        # To classify using the tree:
        # Start with the root as the "current" node
        # As long as the current node is an interior node (type == "split"):
        #    get the value of the attribute the split is performed on
        #    select the child corresponding to that value as the new current node

        # NOTE: In some cases, your tree may not have a child for a particular value
        #       In that case, return the majority value (self.majority) from the training set

        # IMPORTANT: You have to perform this classification *for each* element in x

        predictions = []
        for instance in x:
            current_node = self.tree
            while current_node["type"] == "split":
                attribute = current_node["attribute"]
                value = instance[attribute]
                if value in current_node["children"]:
                    current_node = current_node["children"][value]
                else:
                    current_node = {"type": "class", "class": self.majority}
            predictions.append(current_node["class"])
        return predictions

    # DO NOT CHANGE THE FOLLOWING LINE
    def to_dict(self):
        # DO NOT CHANGE THE PRECEDING LINE
        # change this if you store the tree in a different format
        return self.tree

#HYPERPARAMETER TUNING
import numpy as np
import math

# These are suggested helper functions
# You can structure your code differently, but if you have
# trouble getting started, this might be a good starting point

# Create the decision tree recursively
def make_node(previous_ys, xs, ys, columns, depth=0, max_depth=None, min_samples_split=2, min_impurity_decrease=0.0):
    # WARNING: lists are passed by reference in python
    # If you are planning to remove items, it's better
    # to create a copy first
    columns = columns[:]

    # First, check the three termination criteria:

    # If there are no rows (xs and ys are empty):
    #      Return a node that classifies as the majority class of the parent
    if not xs or not ys:
        return {"type": "class", "class": majority(previous_ys)}

    # If all ys are the same:
    #      Return a node that classifies as that class
    if same(ys):
        return {"type": "class", "class": ys[0]}

    # If there are no more columns left:
    #      Return a node that classifies as the majority class of the ys
    if not columns:
        return {"type": "class", "class": majority(ys)}

    # If max_depth is specified and the current depth reaches max_depth:
    #      Return a node that classifies as the majority class of the ys
    if max_depth is not None and depth >= max_depth:
        return {"type": "class", "class": majority(ys)}

    # If the number of samples is less than min_samples_split:
    #      Return a node that classifies as the majority class of the ys
    if len(ys) < min_samples_split:
        return {"type": "class", "class": majority(ys)}

    # Otherwise:
    # Compute the entropy of the current ys
    current_entropy = entropy(ys)
    best_gain = -1
    best_column = None
    best_splits = None

    # For each column:
    for column in columns:
        splits = {}
        for x, y in zip(xs, ys):
            value = x[column]
            if value not in splits:
                splits[value] = {"xs": [], "ys": []}
            splits[value]["xs"].append(x)
            splits[value]["ys"].append(y)

        # Calculate the entropy of each of the pieces
        # Compute the overall entropy as the weighted sum
        split_entropy = sum(
            len(split["ys"]) / len(ys) * entropy(split["ys"])
            for split in splits.values()
        )
        # The gain of the column is the difference of the entropy before
        #    the split, and this new overall entropy
        gain = current_entropy - split_entropy

        # Select the column with the highest gain
        if gain > best_gain:
            best_gain = gain
            best_column = column
            best_splits = splits

    # If no gain or the gain is less than min_impurity_decrease, return a leaf node
    if best_gain <= 0 or best_gain < min_impurity_decrease:
        return {"type": "class", "class": majority(ys)}

    # Split the data along the column values and recursively call
    #    make_node for each piece
    # Create a split-node that splits on this column, and has the result
    #    of the recursive calls as children.
    node = {"type": "split", "attribute": best_column, "children": {}}
    remaining_columns = [col for col in columns if col != best_column]

    for value, split in best_splits.items():
        node["children"][value] = make_node(
            ys, split["xs"], split["ys"], remaining_columns, depth + 1, max_depth, min_samples_split, min_impurity_decrease
        )

    return node

# Determine if all values in a list are the same
# Useful for the second basecase above
def same(values):
    if not values:
        return True
    # if there are values:
    first_val = values[0]
    # pick the first, check if all other are the same
    return all(val == first_val for val in values)

# Determine how often each value shows up
# in a list; this is useful for the entropy
# but also to determine which values is the
# most common
def counts(values):
    count_dict = {}
    for value in values:
        if value in count_dict:
            count_dict[value] += 1
        else:
            count_dict[value] = 1
    return count_dict

# Return the most common value from a list
# Useful for base cases 1 and 3 above.
def majority(values):
    count_dict = counts(values)
    return max(count_dict, key=count_dict.get)

# Calculate the entropy of a set of values
# First count how often each value shows up
# When you divide this value by the total number
# of elements, you get the probability for that element
# The entropy is the negation of the sum of p*log2(p)
# for all these probabilities.
def entropy(values):
    count_dict = counts(values)
    total = len(values)
    ent = 0
    for count in count_dict.values():
        p = count / total
        ent -= p * math.log2(p)
    return ent

# This is the main decision tree class
# DO NOT CHANGE THE FOLLOWING LINE
class DecisionTree:
    # DO NOT CHANGE THE PRECEDING LINE
    def __init__(self, tree={}, max_depth=None, min_samples_split=2, min_impurity_decrease=0.0):
        self.tree = tree
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_impurity_decrease = min_impurity_decrease

    # DO NOT CHANGE THE FOLLOWING LINE
    def fit(self, x, y):
        # DO NOT CHANGE THE PRECEDING LINE

        self.majority = majority(y)
        self.tree = make_node(y, x, y, list(range(len(x[0]))), max_depth=self.max_depth, min_samples_split=self.min_samples_split, min_impurity_decrease=self.min_impurity_decrease)

    # DO NOT CHANGE THE FOLLOWING LINE
    def predict(self, x):
        # DO NOT CHANGE THE PRECEDING LINE
        if not self.tree:
            return None

        # To classify using the tree:
        # Start with the root as the "current" node
        # As long as the current node is an interior node (type == "split"):
        #    get the value of the attribute the split is performed on
        #    select the child corresponding to that value as the new current node

        # NOTE: In some cases, your tree may not have a child for a particular value
        #       In that case, return the majority value (self.majority) from the training set

        # IMPORTANT: You have to perform this classification *for each* element in x

        predictions = []
        for instance in x:
            current_node = self.tree
            while current_node["type"] == "split":
                attribute = current_node["attribute"]
                value = instance[attribute]
                if value in current_node["children"]:
                    current_node = current_node["children"][value]
                else:
                    current_node = {"type": "class", "class": self.majority}
            predictions.append(current_node["class"])
        return predictions

    # DO NOT CHANGE THE FOLLOWING LINE
    def to_dict(self):
        # DO NOT CHANGE THE PRECEDING LINE
        # change this if you store the tree in a different format
        return self.tree

import pandas as pd

def bin_box_office(value):
    if pd.isna(value):
        return None  # Skip unknown BoxOffice values
    if isinstance(value, float):
        revenue = value
    else:
        revenue = float(value.replace("$", "").replace(",", ""))
    return "Low BoxOffice" if revenue < 70_000_000 else "High BoxOffice"

data = pd.read_csv("movies.csv")
data = data.astype(str)
data.replace('nan', np.nan, inplace=True)
data = data.dropna()

#data['BoxOfficeCategory'] = data['BoxOfficeCategory'].map({"Low BoxOffice": 0, "High BoxOffice": 1})
# Split the 'Genre' column and extract the first genre
data['PrGenre'] = data['Genre'].str.split(',').str[0]

# Handle any missing or NaN values by filling them with a placeholder (optional)
data['PrGenre'] = data['PrGenre'].fillna('Unknown')

print(data['PrGenre'])
data['BoxOfficeCategory'] = data['BoxOffice'].apply(bin_box_office)
data = data.drop(columns=["Title", "imdbID", "Released", "Awards", "Plot", "Director", "Type:", "BoxOffice", "imdbVotes", "Length"])
#datax = data[["Rated", "PrGenre"]]
print(data.dtypes)
display(data)

# Add this function after your existing calculate_performance function
def cross_validate(X, y, k_folds=5):
    fold_size = len(X) // k_folds
    accuracies = []

    for i in range(k_folds):
        # Split data into train and validation
        val_start = i * fold_size
        val_end = (i + 1) * fold_size

        validation_x = X[val_start:val_end]
        validation_y = y[val_start:val_end]

        train_x = X[:val_start] + X[val_end:]
        train_y = y[:val_start] + y[val_end:]

        # Train and evaluate
        classifier = DecisionTree(max_depth=2, min_samples_split=2, min_impurity_decrease=0.1)
        classifier.fit(train_x, train_y)
        predictions = classifier.predict(validation_x)

        accuracy = calculate_performance(validation_y, predictions)
        accuracies.append(accuracy)

    return np.mean(accuracies), np.std(accuracies)

# Then, after your existing model evaluation code, you can call it like this:
cv_mean_accuracy, cv_std_accuracy = cross_validate(X, y)
print(f"Cross-Validation Mean Accuracy: {cv_mean_accuracy:.4f} ± {cv_std_accuracy:.4f}")

# Convert the DataFrame to a NumPy array
data_array = data.values
display(data_array)

X = data_array[:, :-1].tolist()  # All columns except the last one
y = data_array[:, -1].tolist()  # The last column (box_office)

# Assuming X and y are already arrays (features and target)
train_size = int(0.7 * len(X))  # 70% of the data
validation_size = int(0.15 * len(X))  # 15% of the data

# Split into 70% training, 15% validation, and the remaining 15% test
train_x, train_y = X[:train_size], y[:train_size]  # First 70% for training
validation_x, validation_y = X[train_size:train_size+validation_size], y[train_size:train_size+validation_size]  # Next 15% for validation

test_x, test_y = X[train_size+validation_size:], y[train_size+validation_size:]  # Remaining 15% for testing
display(train_x)
display(train_y)
#classifier = DecisionTree()

classifier = DecisionTree(max_depth=20, min_samples_split=4, min_impurity_decrease=0.01)
classifier.fit(train_x, train_y)

# Predict on training and validation data
train_y_hat = classifier.predict(train_x)
validation_y_hat = classifier.predict(validation_x)

# Calculate performance (accuracy)
def calculate_performance(y_true, y_pred):
    correct = sum(yt == yp for yt, yp in zip(y_true, y_pred))
    return correct / len(y_true)

train_accuracy = calculate_performance(train_y, train_y_hat)
#display(validation_y, validation_y_hat)
validation_accuracy = calculate_performance(validation_y, validation_y_hat)

# Print results
print("Training Accuracy:", train_accuracy)
print("Validation Accuracy:", validation_accuracy)
display("Decision Tree Structure:", classifier.to_dict())


#change parameters
#change range of vals
#train k fold

test_y_hat = classifier.predict(test_x)
test_accuracy = calculate_performance(test_y, test_y_hat)
print("Test Accuracy:", test_accuracy)

def evaluate_performance(y_true, y_pred, labels):
    # Initialize confusion matrix
    confusion_matrix = {label: {pred: 0 for pred in labels} for label in labels}

    # Populate the confusion matrix
    for true, pred in zip(y_true, y_pred):
        confusion_matrix[true][pred] += 1

    # Calculate metrics
    tp = {label: confusion_matrix[label][label] for label in labels}  # True positives
    fp = {label: sum(confusion_matrix[other][label] for other in labels if other != label) for label in labels}  # False positives
    fn = {label: sum(confusion_matrix[label][other] for other in labels if other != label) for label in labels}  # False negatives
    tn = {label: sum(sum(confusion_matrix[other][other_pred] for other_pred in labels if other_pred != label)
                     for other in labels if other != label)
          for label in labels}  # True negatives

    precision = {label: tp[label] / (tp[label] + fp[label]) if (tp[label] + fp[label]) > 0 else 0 for label in labels}
    recall = {label: tp[label] / (tp[label] + fn[label]) if (tp[label] + fn[label]) > 0 else 0 for label in labels}

    # Overall accuracy
    accuracy = sum(tp.values()) / sum(sum(row.values()) for row in confusion_matrix.values())

    # Print confusion matrix
    print("Confusion Matrix:")
    for label in labels:
        print(f"{label}: {confusion_matrix[label]}")

    # Print metrics
    print("\nMetrics:")
    print(f"Accuracy: {accuracy:.2f}")
    for label in labels:
        print(f"Class {label}: Precision: {precision[label]:.2f}, Recall: {recall[label]:.2f}, TN: {tn[label]}")

    return confusion_matrix, precision, recall, accuracy

# After training and predicting
train_confusion, train_precision, train_recall, train_accuracy = evaluate_performance(train_y, train_y_hat, labels=set(train_y))
validation_confusion, validation_precision, validation_recall, validation_accuracy = evaluate_performance(validation_y, validation_y_hat, labels=set(validation_y))

# Predict on the test set
test_y_hat = classifier.predict(test_x)

# Evaluate test set performance
test_confusion, test_precision, test_recall, test_accuracy = evaluate_performance(test_y, test_y_hat, labels=set(test_y))

# Print results for training
print("\nTraining Metrics:")
print("Confusion Matrix:", train_confusion)
print("Precision:", train_precision)
print("Recall:", train_recall)
print("Accuracy:", train_accuracy)

# Print results for validation
print("\nValidation Metrics:")
print("Confusion Matrix:", validation_confusion)
print("Precision:", validation_precision)
print("Recall:", validation_recall)
print("Accuracy:", validation_accuracy)

# Print results for test
print("\nTest Metrics:")
print("Confusion Matrix:", test_confusion)
print("Precision:", test_precision)
print("Recall:", test_recall)
print("Accuracy:", test_accuracy)